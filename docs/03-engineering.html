<!DOCTYPE html>
<html>
  <head>
    <title>Feature &amp; Target Engineering</title>
    <meta charset="utf-8">
    <meta name="author" content="Brad Boehmke &amp; Brandon Greenwell" />
    <meta name="date" content="2019-04-01" />
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
    <link rel="stylesheet" href="mtheme_max.css" type="text/css" />
    <link rel="stylesheet" href="fonts_mtheme_max.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: clear, center, middle

background-image: url(images/engineering-icon.jpg)
background-position: center
background-size: cover

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
.font200.bold.white[Feature &amp; Target Engineering]

---
# Introduction

Data pre-processing and engineering techniques generally refer to the .blue[___addition, deletion, or transformation of data___].

.pull-left[

.center.bold.font120[Thoughts]

- Substantial time commitment
- 1 hr module doesn't do justice
- Not a "sexy" area to study but well worth your time
- Additional resources to start with:
   - [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/)
   - [Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241)

]

--

.pull-right[

.center.bold.font120[Overview]

- Target engineering
- Missingness
- Feature filtering
- Numeric feature engineering
- Categorical feature engineering
- Dimension reduction
- Proper implementation

]

---
# Prereqs .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 1]

.pull-left[

.center.bold.font120[Packages]


```r
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
```


]

.pull-right[

.center.bold.font120[Data]


```r
# ames data
ames &lt;- AmesHousing::make_ames()

# split data
set.seed(123)
split &lt;- initial_split(ames, strata = "Sale_Price")
ames_train &lt;- training(split)
```

]

---
class: center, middle, inverse

.font300.white[Target Engineering]

---
# Normality correction

.pull-left[

Not a requirement but...

- can improve predictive accuracy for parametric &amp; distance-based models
- can correct for residual assumption violations
- minimizes effects of outliers

plus...

- sometimes used to for shaping the business problem as well

.center[_“taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.”_]

]

.pull-right[

&lt;br&gt;&lt;br&gt;

&lt;center&gt;
`\(\texttt{Sale_Price} = \beta_0 + \beta_1\texttt{Year_Built} + \epsilon\)`
&lt;/center&gt;

&lt;img src="03-engineering_files/figure-html/skewed-residuals-1.png" style="display: block; margin: auto;" /&gt;


]

---
# Transformation options

.pull-left[

- log (or log with offset)

- Box-Cox: automates process of finding proper transformation

$$
 \begin{equation} 
 y(\lambda) =
`\begin{cases}
   \frac{y^\lambda-1}{\lambda}, &amp; \text{if}\ \lambda \neq 0 \\
   \log y, &amp; \text{if}\ \lambda = 0.
\end{cases}`
\end{equation}`
$$

- Yeo-Johnson: modified Box-Cox for non-strictly positive values

]

.pull-right[

We'll put these pieces together later


```r
step_log()
step_BoxCox()
step_YeoJohnson()
```


]

&lt;img src="03-engineering_files/figure-html/distribution-comparison-1.png" style="display: block; margin: auto;" /&gt;

---
class: center, middle, inverse

.font300.white[Missingness]

.white[_Many models cannot cope with missing data so imputation strategies may be necessary._]

---
# Visualizing .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 2]

An uncleaned version of Ames housing data:


```r
sum(is.na(AmesHousing::ames_raw))
## [1] 13997
```

.pull-left[


```r
AmesHousing::ames_raw %&gt;%
  is.na() %&gt;%
  reshape2::melt() %&gt;%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", labels = c("Present", "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
```


]

.pull-right[

&lt;img src="03-engineering_files/figure-html/missing-distribution-plot-1.png" style="display: block; margin: auto;" /&gt;

]

---
# Visualizing .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 3]

An uncleaned version of Ames housing data:


```r
sum(is.na(AmesHousing::ames_raw))
## [1] 13997
```

.pull-left[


```r
extracat::visna(AmesHousing::ames_raw, sort = "b")
```


]

.pull-right[

&lt;img src="03-engineering_files/figure-html/missing-distribution-plot2-1.png" style="display: block; margin: auto;" /&gt;

]

---
# Structural vs random .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 4]

.pull-left[

Missing values can be a result of many different reasons; however, these reasons are usually lumped into two categories: 

* informative missingess

* missingness at random


]

.pull-right[


```r
AmesHousing::ames_raw %&gt;% 
  filter(is.na(`Garage Type`)) %&gt;% 
  select(`Garage Type`, `Garage Cars`, `Garage Area`)
## # A tibble: 157 x 3
##    `Garage Type` `Garage Cars` `Garage Area`
##    &lt;chr&gt;                 &lt;int&gt;         &lt;int&gt;
##  1 &lt;NA&gt;                      0             0
##  2 &lt;NA&gt;                      0             0
##  3 &lt;NA&gt;                      0             0
##  4 &lt;NA&gt;                      0             0
##  5 &lt;NA&gt;                      0             0
##  6 &lt;NA&gt;                      0             0
##  7 &lt;NA&gt;                      0             0
##  8 &lt;NA&gt;                      0             0
##  9 &lt;NA&gt;                      0             0
## 10 &lt;NA&gt;                      0             0
## # … with 147 more rows
```

]

&lt;br&gt;

.center.bold[Determines how you will, and if you can/should, impute.]

---
# Imputation

.pull-left[

Primary methods:

- Estimated statistic (i.e. mean, median, mode)

- K-nearest neighbor

- Tree-based (bagged trees)

]

.pull-right[

.center.font80[.red[Actual values] vs .blue[imputed values]]

&lt;img src="03-engineering_files/figure-html/imputation-examples-1.png" style="display: block; margin: auto;" /&gt;


]

---
# Imputation

.pull-left[

Primary methods:

- Estimated statistic (i.e. mean, median, mode)

- K-nearest neighbor

- Tree-based (bagged trees)

]

.pull-right[

We'll put these pieces together later


```r
step_meanimpute()
step_medianimpute()
step_modeimpute()
step_knnimpute()
step_bagimpute()
```


]

---
class: center, middle, inverse

.font300.white[Feature Filtering]

---
# More is not always better!

Excessive noisy variables can...

.font120.bold[reduce accuracy]

&lt;img src="images/accuracy-comparison-1.png" width="2560" style="display: block; margin: auto;" /&gt;


---
# More is not always better!

Excessive noisy variables can...

.font120.bold[increase computation time]

&lt;img src="images/impact-on-time-1.png" width="2560" style="display: block; margin: auto;" /&gt;

---
# Options for filtering .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 5]

.pull-left[
Filtering options include:

- removing 
   - zero variance features
   - near-zero variance features
   - highly correlated features (better to do dimension reduction)

- Feature selection
   - beyond scope of module
   - see [Applied Predictive Modeling, ch. 19](http://appliedpredictivemodeling.com/)
]

.pull-right[


```r
caret::nearZeroVar(ames_train, saveMetrics= TRUE) %&gt;% 
  rownames_to_column() %&gt;% 
  filter(nzv)
##               rowname  freqRatio percentUnique zeroVar  nzv
## 1              Street  218.90000    0.09095043   FALSE TRUE
## 2               Alley   22.31522    0.13642565   FALSE TRUE
## 3        Land_Contour   23.05814    0.18190086   FALSE TRUE
## 4           Utilities 2197.00000    0.13642565   FALSE TRUE
## 5          Land_Slope   22.76087    0.13642565   FALSE TRUE
## 6         Condition_2  242.00000    0.31832651   FALSE TRUE
## 7           Roof_Matl  127.47059    0.36380173   FALSE TRUE
## 8           Bsmt_Cond   19.71717    0.27285130   FALSE TRUE
## 9      BsmtFin_Type_2   24.20513    0.31832651   FALSE TRUE
## 10       BsmtFin_SF_2  486.00000    9.54979536   FALSE TRUE
## 11            Heating  103.09524    0.27285130   FALSE TRUE
## 12    Low_Qual_Fin_SF  723.33333    1.22783083   FALSE TRUE
## 13      Kitchen_AbvGr   22.60215    0.18190086   FALSE TRUE
## 14         Functional   40.90000    0.36380173   FALSE TRUE
## 15     Enclosed_Porch  102.72222    7.41246021   FALSE TRUE
## 16 Three_season_porch  723.33333    1.18235562   FALSE TRUE
## 17       Screen_Porch  183.18182    4.77489768   FALSE TRUE
## 18          Pool_Area 2190.00000    0.45475216   FALSE TRUE
## 19            Pool_QC  730.00000    0.22737608   FALSE TRUE
## 20       Misc_Feature   31.22059    0.27285130   FALSE TRUE
## 21           Misc_Val  151.85714    1.40973170   FALSE TRUE
```

]

---
# Options for filtering

.pull-left[
Filtering options include:

- removing 
   - zero variance features
   - near-zero variance features
   - highly correlated features (better to do dimension reduction)

- Feature selection
   - beyond scope of module
   - see [Applied Predictive Modeling, ch. 19](http://appliedpredictivemodeling.com/)
]

.pull-right[

We'll put these pieces together later


```r
step_zv()
step_nzv()
step_corr()
```

]

---
class: center, middle, inverse

.font300.white[Numeric Feature Engineering]

---
# Transformations

.pull-left[
* skewness
   - parametric models that have distributional assumptions (i.e. GLMs, regularized models)
   - log
   - Box-Cox or Yeo-Johnson
   
* standardization
   - Models that incorporate linear functions (GLM, NN) and distance functions (i.e. KNN, clustering) of input features are sensitive to the scale of the inputs 
   - centering _and_ scaling so that numeric variables have `\(\mu = 0; \sigma = 1\)` 
]   
 
.pull-right[

&lt;img src="03-engineering_files/figure-html/standardizing-1.png" style="display: block; margin: auto;" /&gt;
] 

   
---
# Transformations

.pull-left[
* skewness
   - parametric models that have distributional assumptions (i.e. GLMs, regularized models)
   - log
   - Box-Cox or Yeo-Johnson
   
* standardization
   - Models that incorporate linear functions (GLM, NN) and distance functions (i.e. KNN, clustering) of input features are sensitive to the scale of the inputs 
   - centering _and_ scaling so that numeric variables have `\(\mu = 0; \sigma = 1\)` 
] 

.pull-right[

We'll put these pieces together later


```r
step_log()
step_BoxCox()
step_YeoJohnson()
step_center()
step_scale()
```

]

---
class: center, middle, inverse

.font300.white[Categorical Feature Engineering]

---
# One-hot &amp; Dummy encoding

.pull-left[

Many models require all predictor variables to be numeric (i.e. GLMs, SVMs, NNets)

&lt;table class="table table-striped" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; x &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; a &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; c &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; b &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; c &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
   
Two most common approaches include...

]

.pull-right[

.bold.center[Dummy encoding]

&lt;table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x.b &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x.c &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

.bold.center[One-hot encoding]

&lt;table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; id &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x.a &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x.b &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; x.c &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

---
# Label encoding .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 6]

.pull-left[
* One-hot and dummy encoding are not good when:
   - you have a lot of categorical features
   - with high cardinality
   - or you have ordinal features

* Label encoding:
   - pure numeric conversion of the levels of a categorical variable
   - most common: ordinal encoding
]

.pull-right[

.center.bold[Quality variables with natural ordering]


```r
ames_train %&gt;% select(matches("Qual|QC|Qu"))
## # A tibble: 2,199 x 9
##    Overall_Qual Exter_Qual Bsmt_Qual Heating_QC Low_Qual_Fin_SF
##    &lt;fct&gt;        &lt;fct&gt;      &lt;fct&gt;     &lt;fct&gt;                &lt;int&gt;
##  1 Above_Avera… Typical    Typical   Typical                  0
##  2 Good         Good       Typical   Excellent                0
##  3 Average      Typical    Good      Good                     0
##  4 Above_Avera… Typical    Typical   Excellent                0
##  5 Very_Good    Good       Good      Excellent                0
##  6 Very_Good    Good       Good      Excellent                0
##  7 Good         Typical    Typical   Good                     0
##  8 Above_Avera… Typical    Good      Good                     0
##  9 Above_Avera… Typical    Good      Excellent                0
## 10 Good         Typical    Good      Good                     0
## # … with 2,189 more rows, and 4 more variables: Kitchen_Qual &lt;fct&gt;,
## #   Fireplace_Qu &lt;fct&gt;, Garage_Qual &lt;fct&gt;, Pool_QC &lt;fct&gt;
```

]

---
# Label encoding .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 7]

.pull-left[
* One-hot and dummy encoding are not good when:
   - you have a lot of categorical features
   - with high cardinality
   - or you have ordinal features

* Label encoding:
   - pure numeric conversion of the levels of a categorical variable
   - most common: ordinal encoding
]

.pull-right[

.center.bold[Original encoding for `Overall_Qual`]


```r
count(ames_train, Overall_Qual)
## # A tibble: 10 x 2
##    Overall_Qual       n
##    &lt;fct&gt;          &lt;int&gt;
##  1 Very_Poor          3
##  2 Poor              12
##  3 Fair              29
##  4 Below_Average    166
##  5 Average          607
##  6 Above_Average    553
##  7 Good             458
##  8 Very_Good        266
##  9 Excellent         81
## 10 Very_Excellent    24
```

]

---
# Label encoding .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 8]

.pull-left[
* One-hot and dummy encoding are not good when:
   - you have a lot of categorical features
   - with high cardinality
   - or you have ordinal features

* Label encoding:
   - pure numeric conversion of the levels of a categorical variable
   - most common: ordinal encoding
]

.pull-right[

.center.bold[Label/ordinal encoding for `Overall_Qual`]


```r
recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_integer(Overall_Qual) %&gt;%
  prep(ames_train) %&gt;%
  bake(ames_train) %&gt;%
  count(Overall_Qual)
## # A tibble: 10 x 2
##    Overall_Qual     n
##           &lt;dbl&gt; &lt;int&gt;
##  1            1     3
##  2            2    12
##  3            3    29
##  4            4   166
##  5            5   607
##  6            6   553
##  7            7   458
##  8            8   266
##  9            9    81
## 10           10    24
```

]

---
# Common categorical encodings

We'll put these pieces together later


```r
step_dummy()
step_dummy(one_hot = TRUE)
step_integer()
step_ordinalscore()
```

---
class: center, middle, inverse

.font300.white[Dimension Reduction]

---
# PCA

.pull-left[
* We can use PCA for downstream modeling

* In the Ames data, there are potential clusters of highly correlated variables:

   - proxies for size: `Lot_Area`, `Gr_Liv_Area`, `First_Flr_SF`, `Bsmt_Unf_SF`, etc.
   - quality fields: `Overall_Qual`, `Garage_Qual`, `Kitchen_Qual`, `Exter_Qual`, etc.

* It would be nice if we could combine/amalgamate the variables in these clusters into a single variable that represents them.

* In fact, we can explain 95% of the variance in our numeric features with 38 PCs

]

.pull-right[

&lt;img src="03-engineering_files/figure-html/pca-1.png" style="display: block; margin: auto;" /&gt;

]

---
# PCA

.pull-left[
* We can use PCA for downstream modeling

* In the Ames data, there are potential clusters of highly correlated variables:

   - proxies for size: `Lot_Area`, `Gr_Liv_Area`, `First_Flr_SF`, `Bsmt_Unf_SF`, etc.
   - quality fields: `Overall_Qual`, `Garage_Qual`, `Kitchen_Qual`, `Exter_Qual`, etc.

* It would be nice if we could combine/amalgamate the variables in these clusters into a single variable that represents them.

* In fact, we can explain 95% of the variance in our numeric features with 38 PCs

]

.pull-right[

We'll put these pieces together later


```r
step_pca()
step_kpca()
step_pls()
step_spatialsign()
```

]

---
class: center, middle, inverse

.font300.white[Blueprints]

---
# Sequential steps

.pull-left[

.bold.center.font120[Some thoughts to consider]

- If using a log or Box-Cox transformation, don’t center the data first or do any operations that might make the data non-positive. 
- Standardize your numeric features prior to one-hot/dummy encoding.
- If you are lumping infrequently categories together, do so before one-hot/dummy encoding.
- Although you can perform dimension reduction procedures on categorical features, it is common to primarily do so on numeric features when doing so for feature engineering purposes.

]

--

.pull-right[

.bold.center.font120[Suggested ordering]

1. Filter out zero or near-zero variance features
2. Perform imputation if required
3. Normalize to resolve numeric feature skewness
4. Standardize (center and scale) numeric features
5. Perform dimension reduction (i.e. PCA) on numeric features
6. Create one-hot or dummy encoded features

]

---
# Data leakage

___Data leakage___ is when information from outside the training dataset is used to create the model.

- Often occurs when doing feature engineering
- Feature engineering should be done in isolation of each resampling iteration

&lt;img src="images/data-leakage.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;


---
# Putting the process together

.pull-left[
.font120[

* __recipes__ provides a convenient way to create feature engineering blueprints

]
]

.pull-right[

&lt;img src="https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png" width="70%" height="70%" style="display: block; margin: auto;" /&gt;

]

.center.bold.font120[https://tidymodels.github.io/recipes/index.html]

---
# Putting the process together

.pull-left[

* __recipes__ provides a convenient way to create feature engineering blueprints

* 3 main components to consider
   1. recipe: define your pre-processing blueprint
   2. prepare: estimate parameters based on training data
   3. bake/juice: apply blueprint to new data

]

---
# Putting the process together .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 9]

.pull-left[

* __recipes__ provides a convenient way to create feature engineering blueprints

* 3 main components to consider
   1. .bold[recipe: define your pre-processing blueprint]
   2. prepare: estimate parameters based on training data
   3. bake/juice: apply blueprint to new data

&lt;br&gt;

.center.blue[Check out all the available `step_xxx()` functions at http://bit.ly/step_functions]

]

.pull-right[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_integer(matches("Qual|Cond|QC|Qu"))

blueprint
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         80
## 
## Operations:
## 
## Sparse, unbalanced variable filter on all_nominal()
## Centering for all_numeric(), -all_outcomes()
## Scaling for all_numeric(), -all_outcomes()
## Integer encoding for matches("Qual|Cond|QC|Qu")
```

]

---
# Putting the process together .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 10]

.pull-left[

* __recipes__ provides a convenient way to create feature engineering blueprints

* 3 main components to consider
   1. recipe: define your pre-processing blueprint
   2. .bold[prepare: estimate parameters based on training data]
   3. bake/juice: apply blueprint to new data

]

.pull-right[


```r
prepare &lt;- prep(blueprint, training = ames_train)
prepare
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         80
## 
## Training data contained 2199 data points and no missing data.
## 
## Operations:
## 
## Sparse, unbalanced variable filter removed Street, Alley, ... [trained]
## Centering for Lot_Frontage, Lot_Area, ... [trained]
## Scaling for Lot_Frontage, Lot_Area, ... [trained]
## Integer encoding for Condition_1, Overall_Qual, Overall_Cond, ... [trained]
```

]

---
# Putting the process together .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 11]

.scrollable90[
.pull-left[

* __recipes__ provides a convenient way to create feature engineering blueprints

* 3 main components to consider
   1. recipe: define your pre-processing blueprint
   2. prepare: estimate parameters based on training data
   3. .bold[bake: apply blueprint to new data]

]

.pull-right[


```r
baked_train &lt;- bake(prepare, new_data = ames_train)
baked_test &lt;- bake(prepare, new_data = ames_test)

baked_train
## # A tibble: 2,199 x 68
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Lot_Shape Lot_Config
##    &lt;fct&gt;       &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     
##  1 One_Story_… Resident…       0.692   0.515   Slightly… Corner    
##  2 One_Story_… Resident…       1.05    0.125   Regular   Corner    
##  3 Two_Story_… Resident…       0.484   0.460   Slightly… Inside    
##  4 Two_Story_… Resident…       0.603  -0.0227  Slightly… Inside    
##  5 One_Story_… Resident…      -0.496  -0.656   Regular   Inside    
##  6 One_Story_… Resident…      -0.436  -0.646   Slightly… Inside    
##  7 Two_Story_… Resident…       0.0682 -0.333   Regular   Inside    
##  8 Two_Story_… Resident…       0.513  -0.0199  Slightly… Corner    
##  9 One_Story_… Resident…      -1.71   -0.273   Slightly… Inside    
## 10 One_Story_… Resident…       0.810   0.00212 Regular   Inside    
## # … with 2,189 more rows, and 62 more variables: Neighborhood &lt;fct&gt;,
## #   Condition_1 &lt;dbl&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;,
## #   Overall_Qual &lt;dbl&gt;, Overall_Cond &lt;dbl&gt;, Year_Built &lt;dbl&gt;,
## #   Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;fct&gt;, Exterior_1st &lt;fct&gt;,
## #   Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;,
## #   Exter_Qual &lt;dbl&gt;, Exter_Cond &lt;dbl&gt;, Foundation &lt;fct&gt;, Bsmt_Qual &lt;dbl&gt;,
## #   Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;,
## #   BsmtFin_SF_2 &lt;dbl&gt;, Bsmt_Unf_SF &lt;dbl&gt;, Total_Bsmt_SF &lt;dbl&gt;,
## #   Heating_QC &lt;dbl&gt;, Central_Air &lt;fct&gt;, Electrical &lt;fct&gt;,
## #   First_Flr_SF &lt;dbl&gt;, Second_Flr_SF &lt;dbl&gt;, Low_Qual_Fin_SF &lt;dbl&gt;,
## #   Gr_Liv_Area &lt;dbl&gt;, Bsmt_Full_Bath &lt;dbl&gt;, Bsmt_Half_Bath &lt;dbl&gt;,
## #   Full_Bath &lt;dbl&gt;, Half_Bath &lt;dbl&gt;, Bedroom_AbvGr &lt;dbl&gt;,
## #   Kitchen_AbvGr &lt;dbl&gt;, Kitchen_Qual &lt;dbl&gt;, TotRms_AbvGrd &lt;dbl&gt;,
## #   Fireplaces &lt;dbl&gt;, Fireplace_Qu &lt;dbl&gt;, Garage_Type &lt;fct&gt;,
## #   Garage_Finish &lt;fct&gt;, Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;,
## #   Garage_Qual &lt;dbl&gt;, Garage_Cond &lt;dbl&gt;, Paved_Drive &lt;fct&gt;,
## #   Wood_Deck_SF &lt;dbl&gt;, Open_Porch_SF &lt;dbl&gt;, Enclosed_Porch &lt;dbl&gt;,
## #   Three_season_porch &lt;dbl&gt;, Screen_Porch &lt;dbl&gt;, Pool_Area &lt;dbl&gt;,
## #   Fence &lt;fct&gt;, Misc_Val &lt;dbl&gt;, Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;,
## #   Sale_Type &lt;fct&gt;, Sale_Condition &lt;dbl&gt;, Sale_Price &lt;int&gt;,
## #   Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;
```

]
]

---
# Simplifying with __caret__

.pull-left[

* __recipes__ provides a convenient way to create feature engineering blueprints

* 3 main components to consider
   1. recipe: define your pre-processing blueprint
   2. prepare: estimate parameters based on training data
   3. bake: apply blueprint to new data
   
* Luckily, __caret__ simplifies this process for us.
   1. We supply __caret__ a recipe
   2. __caret__ will prepare &amp; bake within each resample 

]

.pull-right[

&lt;br&gt;

&lt;img src="https://media1.tenor.com/images/6358cb41e076a3c517e5a9988b1dc888/tenor.gif?itemid=5711499" width="90%" height="90%" style="display: block; margin: auto;" /&gt;


]

---
# Putting the process together .red[<span>&lt;i class="fas  fa-hand-point-right faa-horizontal animated " style=" color:red;"&gt;&lt;/i&gt;</span> code chunk 12]

.scrollable90[
.pull-left[
Let's add a blueprint to our modeling process for analyzing the Ames housing data:

1. Split into training vs testing data

2. .blue[Create feature engineering blueprint]

3. Specify a resampling procedure

4. Create our hyperparameter grid

5. Execute grid search

6. Evaluate performance
]

.pull-right[

.center.bold[<span>&lt;i class="fas  fa-exclamation-triangle faa-FALSE animated " style=" color:red;"&gt;&lt;/i&gt;</span>  This grid search takes ~8 min <span>&lt;i class="fas  fa-exclamation-triangle faa-FALSE animated " style=" color:red;"&gt;&lt;/i&gt;</span>]


```r
# 1. stratified sampling with the rsample package
set.seed(123)
split  &lt;- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  &lt;- training(split)
ames_test   &lt;- testing(split)

# 2. Feature engineering
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;%
  step_integer(matches("Qual|Cond|QC|Qu")) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)

# 3. create a resampling method
cv &lt;- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# 4. create a hyperparameter grid search
hyper_grid &lt;- expand.grid(k = seq(2, 25, by = 1))

# 5. execute grid search with knn model
#    use RMSE as preferred metric
knn_fit &lt;- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# 6. evaluate results
# print model results
knn_fit
## k-Nearest Neighbors 
## 
## 2054 samples
##   80 predictor
## 
## Recipe steps: nzv, integer, center, scale, dummy 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 1849, 1849, 1849, 1848, 1850, 1848, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE     
##    2  36404.68  0.7944128  22482.91
##    3  35319.96  0.8073602  21554.09
##    4  35424.81  0.8054775  21361.14
##    5  35214.10  0.8100148  21163.72
##    6  34645.72  0.8183326  20894.01
##    7  34409.24  0.8220020  20832.48
##    8  34023.75  0.8275806  20669.88
##    9  33818.08  0.8312492  20596.20
##   10  33744.59  0.8326048  20624.06
##   11  33734.82  0.8337820  20623.66
##   12  33723.32  0.8348085  20606.53
##   13  33794.99  0.8347543  20671.04
##   14  33972.82  0.8341336  20765.06
##   15  34075.31  0.8336435  20809.05
##   16  34150.00  0.8339415  20853.42
##   17  34203.56  0.8341864  20940.34
##   18  34284.83  0.8337899  21012.43
##   19  34325.85  0.8337150  21063.53
##   20  34381.42  0.8333470  21140.70
##   21  34424.06  0.8332503  21173.41
##   22  34443.72  0.8334388  21195.66
##   23  34489.65  0.8335386  21225.77
##   24  34509.98  0.8335924  21241.38
##   25  34532.88  0.8338991  21275.36
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 12.

# plot cross validation results
ggplot(knn_fit$results, aes(k, RMSE)) + 
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = scales::dollar)
```

&lt;img src="03-engineering_files/figure-html/example-blue-print-application-1.png" style="display: block; margin: auto;" /&gt;

]
]

---
# Putting the process together

.center.bold.font120[Feature engineering alone reduced our error by $10,000!]

&lt;img src="https://media1.tenor.com/images/2b6d0826f02a9ba7c9d4384a740013e9/tenor.gif?itemid=5531028" width="90%" height="90%" style="display: block; margin: auto;" /&gt;

---
# Questions?

&lt;img src="http://www.whitehouse51.com/thumbnail/a/any-questions-meme-100-images-thanks-for-listening-any-1.jpeg" width="50%" height="50%" style="display: block; margin: auto;" /&gt;

---
# Back home

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
[.center[<span>&lt;i class="fas  fa-home fa-10x faa-FALSE animated "&gt;&lt;/i&gt;</span>]](https://github.com/koalaverse/AnalyticsSummit19)

.center[https://github.com/koalaverse/AnalyticsSummit19]
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
